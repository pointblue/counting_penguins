---
title: "CMI Penguin Pipeline"
output: html_notebook
editor_options: 
  chunk_output_type: inline
  markdown: 
    wrap: 80
---

[![](https://conservationmetrics.com/wp-content/uploads/conservation_metrics_350px-01.png "Conservation Metrics"){width="350"}![](https://data.pointblue.org/apps/assets/images/pb-logo-full.png "Point Blue"){width="250"}](https://www.conservationmetrics.com)

This notebook walks a user through using modern machine learning object
detection to count penguins in drone imagery. This is a collaborative work
between [Conservation Metrics](https://www.conservationmetrics.com "CMI") and
[Point Blue Conservation Science](https://www.pointblue.org "PointBlue") as part
of NSF project \#...

# Let's Detect Some Penguins!

This notebook documents the steps needed to prepare a new orthomosaic image for
penguin prediction, runs prediction and post-processing resulting in a shape
file of penguin center points.

1.  [Prepocessing]

2.  [Prediction]

3.  [Postprocessing]

This workflow moves back and forth between `Python` and `R` and this notebook is
an attempt to work seamlessly with both languages. This notebook was tested on
Windows 10 Home Edition and Windows Server 2019. It should be possible to run on
Linux Ubuntu and macOS.

**If you are attempting to run this notebook on linux Ubuntu, there are some
extra steps to install and prep the R environment, especially for the spatial
packages. To install the spatial package** `sf` **you must follow special
instructions [here](https://r-spatial.github.io/sf/#ubuntu "Install `sf`").**

## Set up R Environment

```{r clone yolo repo}
# Load packages -----------------------------------------------------------

install_r_pkgs=FALSE

if(install_r_pkgs){
    packages<-c("reticulate","git2r","purrr","data.table","stringr","dplyr",
                "lubridate","tidyr","ggplot2",'rmarkdown',"exifr",
                "sf","raster","rgdal","RStoolbox","nngeo")

    install.packages(packages)
}

# tidy data
library(data.table)
library(stringr)
library(dplyr)
library(lubridate)
library(tidyr)
library(purrr)

# Plotting
library(ggplot2)

# Spatial
library(sf)
library(nngeo)

# update reticulate if too old
if(!packageVersion("reticulate")>="1.19"){
    install.packages("reticulate")
}

# clone yolov5/penguin-pipe repo
if(!dir.exists("yolov5")){
    # clone the repo
    git2r::clone(url = "https://github.com/ConservationMetrics/yolov5.git",
                 local_path = "yolov5",branch = "penguin-pipe")
}

```

## Setup Python Environment

We are going to install
[miniconda](https://docs.conda.io/en/latest/miniconda.html) and use it to create
a `python` virtual environment so that we do not mess up your existing
environment. `Miniconda` is a light weight version of
[`Anaconda`](https://www.anaconda.com/products/individual), a `python` software
development suite that has package management and `python` versioning.

We are going to be using the `reticulate` package for working with `python`. We
will use a `conda` virtual environment to avoid conflicts with native installs
of python. If there are errors or issues running this code chunk: Restart R, and
run this line by line within the if statement checking for `penpipe`.

```{r setup env}
# Set up python environment
library(reticulate)

# is python installed?
if(miniconda_path()=="" | !dir.exists(miniconda_path())){
    install_miniconda()
}

# what python program is it using?
message("python: ", py_exe())

# check if penpipe environment exists and create if not
if(!"penpipe"%in%conda_list()$name){
    # conda_remove("penpipe")
    conda_create("penpipe",
                 packages = "python=3.8")
    use_condaenv("penpipe",required = T)

    conda_install("penpipe",packages = "pathlib")
    conda_install("penpipe",packages = "argparse", pip = T, python_version = 3.8)
    conda_install("penpipe",packages = "joblib")
    conda_install("penpipe",packages = "tifffile",pip=T)
    conda_install("penpipe",packages = "imagecodecs",pip=T)
    conda_install("penpipe",packages = "scikit-image",pip=T)

    pac<-readLines("yolov5/requirements.txt")
    pac<-pac[!grepl("#",pac)&nchar(pac)>0]
    py_install(pip = T,envname = "penpipe",packages = pac)
    py_install("install matplotlib==3.2",pip=T,envname = "penpipe")

}

# tell r/reticulate what python to use
use_condaenv("penpipe",required = T)

# Check the python version (it should be 3.7.10)
py_run_string("import sys;print(sys.version)")

# check the path to the exicutable to assure it is coming from your penpipe conda environment
py_exe()

```

# Prepocessing

This workflow assumes you have a one or more new orthomosaics ready to tile.
They should be named "smartly" (e.g. `croz_20200112_east_v7.tif`) and without
any spaces or special characters and organized into subfolders if needed.

## Tile Images

To tile imagery we assume that you have already organized the images and named
them with logical names. A key assumption here is that all the images (orthos)
have unique names (so that if you wanted you could drop them all into the same
folder without any conflicts.

### Tile a Directory of Images

The `tile_image_dir_par` function finds all the images (orthos) in a folder and
creates tiles with a specific set of dimensions. This function is parallelized
to run faster, but still takes \~10-90 minutes per orthomosaic. There are
limitations here in terms of memory, so with large orthomosaics it is highly
likely that you will not be able to use the parallel processing. For a typical
uncompressed tif file with RGB and alpha channels, you will need \~3-4x more
memory than the size of the file. so to process a 8 GB file you will need \~32
GB of RAM.

```{python tile images}

# Allow the use of the tiling function (import into the current environment)
import sys

path_to_scripts_folder='D:/git_repos/cmi-penguin-pipeline/scripts/'
# path_to_scripts_folder='C:/Users/cmi/cmi-penguin-pipeline/scripts/'
# path_to_scripts_folder='/home/cmi/cmi-penguin-pipeline/scripts/'

sys.path.insert(0, path_to_scripts_folder )

# load the image tiling functions
from tile_image import tile_image_dir_par

## tile a directory of images
tile_image_dir_par(
    in_folder = "./test_images", # relitive or full path to the folder to tile
    out_folder = "./test_images_tiled", #output folder
    jpg_quality = 100, #jpg image quality
    tile_x = 512, # tile size in x dimension in pixels
    tile_y = 256, # tile size in y dimension in pixels
    object_size_px = 30, # size of an average object in pixels (this sets the overlap to 0/5*object_size_px)
    file_ext = "tif", #file extention of the original image
    out_file_ext = "JPG", # file extention for the tiles
    par=False) #whether or not to use parallel processing (set to spawn 3*cpu_count threads)

```

# Prediction

We provide three single class models for detection of adult penguins incubating
and standing, and chicks. These models are built using [Ultralytics'
YOLOv5](https://github.com/ultralytics/yolov5 "YOLOv5") repo. These models were
trained on RGB JPEG tiles (256x512 pixels). The YOLOv5 architecture provides
bounding boxes and confidence scores for each detected object.

## Run Model

```{python}
# User inputs ---------------------------------------------------------------

# model paths relitive to the YOLOv5 Directory
models=['../models/adult_s2_best.pt',
        '../models/adult_stand_s5_best.pt',
        '../models/chick_s_best.pt']

# tile folder relitive to the YOLOv5 directory
tile_dir= "../test_images_tiled/*"

# confidence threshold
conf_thres=0.01

# largest image dimention
image_size=512

# name for output directory (will be created in the yolov5 directory, e.g. ./yolov5/penguin_2021)
output_dir="penguin_2021"

# load librarys
from pathlib import Path
from argparse import Namespace
import os
# change to the yolov5 directory to inport the detect function
os.chdir("./yolov5")
os.getcwd()
from detect import detect

# loop to run each model
for mod in models:
    print(mod)
    
    # run_name is the name of the subfodler to create within the project folder.
    # In this case we name it after the model.  The detect function is set up to
    # NOT overwrite a run_name if it already exists and will chreate a new name
    # by adding a number after the runname and incrementing, so if you have many
    # runs they will not over write eachouther for better or worse!
    run_name=Path(mod).name.split(sep=".")[0]
    
    # load all the parameters See the detect.py for more info on these
    opt=Namespace(agnostic_nms=False,
        augment=False,
        classes=None,
        conf_thres=conf_thres,
        device="cpu",
        exist_ok=False,
        img_size=image_size,
        iou_thres=0.3,
        name=run_name,
        project=output_dir,
        save_conf=True,
        save_txt=True,
        source=tile_dir,
        update=False,
        view_img=False,
        weights=mod,
        save_xxyy=False)
    
    detect(opt)  

# change back to the main repo directory
os.chdir("..")


```

# Postprocessing {data-link="Postprocessing"}

Before we manipulate the files created during prediction, we must prepare some
info about the files. Specifically we will extract the exif data from the tiles
and we will get the spatial projection/extent info from the original GeoTiff
orthomosaics.

### Read the `exif` data from the tiles

```{r}
# make a location to store these results
dir.create("R_Output/shps",recursive = T,showWarnings = F)

library(exifr)

tile_dir<-py$tile_dir %>% gsub("\\.\\./|/\\*","",.)
exif<-read_exif(tile_dir,recursive = T)
head(exif)
fwrite(exif,"R_Output/tiles_exif_data.csv")

```

### Get the extents from the orthomosaics

```{r}

library(raster)
files<- list.files("test_images/",pattern = "tif",full.names = T,recursive = T)

res<-map_dfr(files,function(file){
  #load the ortho
  temp<-raster(file)
  # build the dataframe with projection,exten,and res info
  data.frame(image_path=file,
             xmin=temp@extent@xmin,
             xmax=temp@extent@xmax,
             ymin=temp@extent@ymin,
             ymax=temp@extent@ymax,
             xres=xres(temp),
             yres=yres(temp),
             proj4=proj4string(temp),
             stringsAsFactors = F)
})
res
saveRDS(res,"R_Output/ortho_extents.rds")

detach("package:raster", unload = TRUE)
```

## Transform Tile Coordinates into Spatial Coordinates

#### Define [Intersection-Over-Union](https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/) (IoU) function

[IoU](https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/)
is a common technique to evaluate "fit" for a predicted bounding box and a
ground truth bounding box. It is also a great way to remove "extra" detection
that might occur for the same object (i.e. two boxes are predicted around the
same object).

```{r iou funciton}

#' IntersectionOver Union for Bounding Boxes
#'
#' Compute the Intersection over Unionfor bounding boxes. Input shold be a
#' single row of a dataframe for both boxA and a second single row for boxB with
#' xmin, xmax,ymin, ymax columns
#'
#'
#' @param boxA Path to an image
#' @param boxB number of pixels to buffer the bbox on all sides
#' @param type if "image" use columns with the prefix image_*.  if other use columns with no prefix
#' @return numeric value of the intersection area devided by the area of the
#'   union between the two boxes
#' @export
#' @author Abram Fleishman \email{abram@@conservationmetics.com}
# @examples 
# boxA<-data.frame(xmin=0,xmax=2,ymin=0,ymax=2)
# boxB<-data.frame(xmin=0,xmax=4,ymin=1,ymax=2)
# iou(boxA,boxB)
# ggplot()+
#   annotate(geom="rect",xmin = boxA$xmin,xmax = boxA$xmax,ymin = boxA$ymin,ymax = boxA$ymax,fill="NULL",col="red")+
#   annotate(geom="rect",xmin = boxB$xmin,xmax = boxB$xmax,ymin = boxB$ymin,ymax = boxB$ymax,fill="NULL",col="blue")


iou<-function(boxA, boxB,type = "other"){
  if(type=="image"){
    xA = pmax(boxA$image_xmin, boxB$image_xmin)
    yA = pmin(boxA$image_ymin, boxB$image_ymin)
    xB = pmin(boxA$image_xmax, boxB$image_xmax)
    yB = pmax(boxA$image_ymax, boxB$image_ymax)
    
    # compute the area of intersection rectangle
    interArea = pmax(0, xB - xA ) * pmax(0, yA - yB )
    
    # compute the area of both the prediction and ground-truth
    # rectangles
    boxAArea = (boxA$image_xmax - boxA$image_xmin ) * (boxA$image_ymin - boxA$image_ymax )
    boxBArea = (boxB$image_xmax - boxB$image_xmin ) * (boxB$image_ymin - boxB$image_ymax )
    
    # compute the intersection over union by taking the intersection
    # area and dividing it by the sum of prediction + ground-truth
    # areas - the interesection area
    interArea / ((boxAArea + boxBArea) - interArea)
  }else{
    # determine the (x, y)-coordinates of the intersection rectangle
    xA = pmax(boxA$xmin, boxB$xmin)
    yA = pmax(boxA$ymin, boxB$ymin)
    xB = pmin(boxA$xmax, boxB$xmax)
    yB = pmin(boxA$ymax, boxB$ymax)
    
    # compute the area of intersection rectangle
    interArea = pmax(0, xB - xA ) * pmax(0, yB - yA )
    
    # compute the area of both the prediction and ground-truth
    # rectangles
    boxAArea = (boxA$xmax - boxA$xmin ) * (boxA$ymax - boxA$ymin )
    boxBArea = (boxB$xmax - boxB$xmin ) * (boxB$ymax - boxB$ymin )
    
    # compute the intersection over union by taking the intersection
    # area and dividing it by the sum of prediction + ground-truth
    # areas - the interesection area
    interArea / ((boxAArea + boxBArea) - interArea)
  }
}
```

### Make image coordinates into spatial coordinates

The detection process is not spatially aware so we must transform the image
pixel coordinates into spatial coordinates. To do this, we use the original
orthomosaic projection, extent, and resolution to offset and project the
bounding boxes.

#### Compile the Detection Files

If there is more than one detection file, we combine them and save them as a
csv.

```{r}
# make image extent table -------------------------------------------------

exts<-readRDS("R_Output/ortho_extents.rds") %>%
    mutate(image_name=gsub(".tif",".JPG",basename(image_path)))


# read labels  --------------------------------------------------

ul<-data.table::fread("R_Output/tiles_exif_data.csv") %>%
    mutate(filename=basename(SourceFile),folder=dirname(SourceFile)) %>%
    select(filename,folder) %>% 
    distinct %>% 
    filter(!is.na(folder)) 

lab_dir=c(file.path("yolov5",py$output_dir))

paths<-list.files(lab_dir,full.names = T,recursive = T,pattern = ".csv")

names(paths)<-paths

dat<-map_dfr(paths,data.table::fread,.id="csv_path") %>%
    mutate(model=basename(dirname(dirname(csv_path)))) %>%
    left_join(ul)

dat<-dat %>% 
    distinct(filename,class,xmin,xmax, ymin,ymax, 
             height, width, box_confidence,model,
             .keep_all = T)

data.table::fwrite(dat,paste0("R_Output/predictions.csv"))

```

#### Convert Tile coordinates to Image Coordinates

The first step is to adjust from tile coordinates to image coordinates by
offsetting the tiles coordinates by the location of the tile in the larger
image.

```{r}


image_ext = "JPG"
filename_parts = c("bare_filename", "crop_x", "crop_y")

# take ext and make it a regex at the end of the string
image_ext_regex<-paste0("\\.",image_ext,"$")

## alternate method fo extracting crop
crop_regex<-paste0("[0-9\\.]{1,}_[0-9\\.]{1,}",image_ext_regex)
dat$crops<-stringr::str_extract(dat$filename,crop_regex) %>%
    gsub(image_ext_regex,"",.)
dat$image_name_bare<- gsub(paste0("_", crop_regex),"",dat$filename)

dat<- separate(dat,col = crops,into = c("crop_x","crop_y"),sep = "_")

dat<-dat %>% 
    mutate (temp_filename=gsub(image_ext_regex,"",filename)) %>% 
    rename("tile_name"="filename") %>% 
    mutate(crop_y=as.numeric(crop_y),
           crop_x=as.numeric(crop_x),
           image_name = paste0(image_name_bare,".JPG"), # not generalized
           xmin=ifelse(class=="NULL",0,xmin),
           ymin=ifelse(class=="NULL",0,ymin),
           xmax=ifelse(class=="NULL",1,xmax),
           ymax=ifelse(class=="NULL",1,ymax),
           image_xmin = (xmin*width)+crop_x,
           image_xmax = (xmax*width)+crop_x,
           image_ymin = (ymin*height)+crop_y,
           image_ymax = (ymax*height)+crop_y) 


head(dat)
head(exts)


```

#### Make Spatial Coordinates from the Image Coordinates

We use the information about the projection and extent of the original
orthomosaics to transform the coordinates.

```{r}

# Make image coords spatial coords ----------------------------------------

images<-unique(dat$image_name)

dat<-map_dfr(images,function(i){
    
    dat_sub<-filter(dat,image_name==i) %>%
        mutate(image_xmin = (xmin*width)+crop_x,
               image_xmax = (xmax*width)+crop_x,
               image_ymin = (ymin*height)+crop_y,
               image_ymax = (ymax*height)+crop_y)
    exts_sub<-filter(exts,image_name==i)
    
    dat_sub$image_xmin<-exts_sub$xmin+(dat_sub$image_xmin*exts_sub$xres)
    dat_sub$image_xmax<-exts_sub$xmin+(dat_sub$image_xmax*exts_sub$xres)
    dat_sub$image_ymin<-exts_sub$ymax-(dat_sub$image_ymin*exts_sub$yres)
    dat_sub$image_ymax<-exts_sub$ymax-(dat_sub$image_ymax*exts_sub$yres)
    
    return(dat_sub)
})
```

## Filter Predictions

We are going to run a version of [non-maximum
suppression](https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c)
(NMS) where each set of events that overlaps with an IoU value of \>0.3 will be
reduced to a single box per class. We will keep the bounding boxes with the
highest box_confidence score for each class. To do this efficiently, we split
each image into a grid and apply NMS to each grid cell (to avoid comparing every
box to every other box across a huge area). This speeds things up a lot with
large images with many boxes

### Create an Image grid

The image grid speeds up NMS a lot by reducing the number of boxes that must be
compared to each other. We have found a cell size of 75 seems to work well (we
are working in geographic image [full ortho] coordinates here).

```{r}
## create event ID ---------------------------------------------------------
### Create image grid------------------------------------------
cell_size=75

img_grid<-map_dfr(1:length(images),function(j){
    dat_sub<-dat %>%filter(image_name==images[j])
    xmn<-min(dat_sub$image_xmin)
    xmx<-max(dat_sub$image_xmin)
    xdiff<-xmx-xmn
    
    ymn<-min(dat_sub$image_ymin)
    ymx<-max(dat_sub$image_ymin)
    ydiff<-ymx-ymn
    
    expand.grid(x=seq(floor(xmn),ceiling(xmx)+cell_size,by = cell_size),
                y=seq(floor(ymn),ceiling(ymx)+cell_size,by = cell_size))%>% 
        mutate(x2=x+cell_size,
               y2=y+cell_size,
               xcen=((x2-x)/2)+x,
               ycen=((y2-y)/2)+y)%>% 
        mutate(image_name = images[j])
    
})

```

### Create event IDs

Events are defined as any set of bounding boxes that overlaps in space on a
single orthomosaic and has a IoU value \>0.3. This step assigns each box to an
event, but retains all the events. **This can take several hours for large
datasets.**

```{r}
### Create event ids ------------------------------------------------------------
iou_threshold = 0.3
iou_type = "image"

# add key if missing
if(!"key"%in%names(dat)){
    dat<-dat %>%
        ungroup %>% 
        mutate(key=row_number()) 
}

singles<-dat %>% 
    group_by(image_name) %>%
    filter(n()==1) %>% 
    mutate(event=1)

if(nrow(singles)>0){
    dat<-dat %>%
        anti_join(singles,by = names(dat))
    img_grid<-img_grid[img_grid$image_name%in%unique(dat$image_name),] 
}

if(nrow(img_grid)>0){
    event_props<- pmap_dfr(
        list(img_grid$x,img_grid$y,
             img_grid$x2,img_grid$y2,
             img_grid$image_name),
        
    function(xi,yi,x2i,y2i,i){
        temp<-dat %>% 
            filter(
                image_name==i,
                (image_xmin>=xi&image_xmin<=x2i&image_ymin>=yi&image_ymin<=y2i)|
                (image_xmin>=xi&image_xmin<=x2i&image_ymax>=yi&image_ymax<=y2i)|
                (image_xmax>=xi&image_xmax<=x2i&image_ymin>=yi&image_ymin<=y2i)|
                (image_xmax>=xi&image_xmax<=x2i&image_ymax>=yi&image_ymax<=y2i)
            )
        
        if(nrow(temp)>0){
            map_dfr(1:nrow(temp),
                    function(j){
                        data.frame(
                            # key=temp$key[j],
                            # key2=temp$key,
                            # iou=iou(temp[j,],temp),
                            origins=paste( 
                                sort(
                                    temp$key[ 
                                        which(
                                            iou(
                                                temp[j, ], 
                                                temp, 
                                                type = iou_type)>=iou_threshold)]),
                                collapse=', '),
                            stringsAsFactors = F)
                    }
            ) 
        }else{
            data.frame(origins=character(),stringsAsFactors = F)
        }
    }
    ) %>%
        distinct %>%
        ungroup %>% 
        transmute(event=row_number(),
                  key= str_split(string = origins,
                                 pattern = ', '))%>%
        unnest(cols = c(key)) %>% 
        mutate(key = as.numeric(key))
    
    print("iou complete")
    event_dup<-event_props %>%
        group_by(key) %>% 
        filter(n()>1) %>% 
        as.data.table()
    
    if(nrow(event_dup)>0){
        
        setkey(event_dup,"event")
        
        k<-unique(event_dup$key)
        
        events<-map_dfr(k,function(i){
            
            evt<-unique(event_dup$event[event_dup$key==i]  )
            
            data.frame(event=min(evt),
                       key=unique(event_dup$key[event_dup$event%in%evt]))
        }) %>%
            group_by(key) %>% 
            filter(event==min(event)) %>%
            distinct %>% 
            bind_rows(event_props %>%
                          anti_join(event_dup,by = c("event", "key")))
        
    }else{
        events<-event_props
    }
}else{
    events<-data.frame(event=integer(),key=integer())
}


ev<-dat %>%
    left_join(events,by = "key") %>%
    bind_rows(singles) %>% 
    mutate(lon=((image_xmax-image_xmin)/2)+image_xmin,
           lat=((image_ymin-image_ymax)/2)+image_ymax,
           ortho_name=gsub("_tiled-[0-9]{1,}-[0-9]{1,}","",image_name),
           delta_x=xmax-xmin,
           delta_y=ymax-ymin,
           aspect_x=delta_x/delta_y,
           aspect_y=delta_y/delta_x)

saveRDS(ev,"R_Output/prediction_events.rds")

```

### Filter Dataset

Now that boxes are grouped into events we can begin to filter the dataset. Below
we implement several filters. Other filters or more sophisticated modeling might
be able to leverage multiple boxes within a single event but that is beyond the
scope of this work.

Filter boxes that meet the following criteria:

-   remove below the box confidence threshold for each model

-   retain boxes with the highest confidence score for each event and class

-   remove chick predictions from images taken before chicks were present

-   remove boxes that are too small or large

```{r}
# load the events and the threshold table
ev<-readRDS("R_Output/prediction_events.rds")
thresh<-read.csv("data/top_models.csv")

top_per_class_ev<-ev %>% 
    group_by(event,class,tile_name) %>% 
    filter(box_confidence==max(box_confidence),class!="NULL") 

no_chicks<-c("royds_20191204_v4.JPG",
             "croz_20201129_v1.JPG",
             "royds_20201201_v1.JPG", 
             "croz_20191121_WB_Area.JPG",
             "croz_20191202_east_v7.JPG",        
             "croz_20191202_west_v5.JPG",
             "croz_20191211_east_v2.JPG",         
             "croz_20191211_west_v2.JPG",
             "croz_20191217_east_v2.JPG",        
             "croz_20191217_east_v3.JPG",
             "croz_20191217_west_v2.JPG",        
             "croz_20191217_west_v3.JPG")

top_per_class_ev2<-top_per_class_ev %>% 
    left_join(thresh %>% 
                  dplyr::select(cutoff,class)) %>% 
    filter(box_confidence>=cutoff) %>% 
    ungroup %>% 
    filter(!(ortho_name %in% no_chicks&class=="ADPE_j"),
           delta_x<0.15, # must be less than 75 pixels
           delta_x<0.3, # must be less than 75 pixels
           delta_x>=(6/500), #filter out boxes <6 pixels
           delta_y>=(6/256))

top_per_ev3<-top_per_class_ev2 %>% 
    group_by(event,image_name) %>% 
    filter(box_confidence==max(box_confidence)) %>% 
  distinct(lat,lon,event,image_name,class, .keep_all = T) #there are duplicate rows! This removes them (but what are there duplicates?)
```
### Filter Dataset 2: Remove double counts

There are some double detections resulting from the sliding window cutting penguins in half.  Here we calculate the distance to the nearest neighbor for each detection and for pairs where the center points are within 0.33m we remove the detection with the lower box confidence score.  This works well for the penguin adult classes, but probably will not work for the chick images.   


```{r}
images=unique(top_per_ev3$ortho_name)

top_per_ev_sf<-top_per_ev3 %>% 
    st_as_sf(coords=c("lon","lat"))


top_per_ev_sf_nn<-map_dfr(images,function(x){
    # Set projection for each image
    temp_ortho<-top_per_ev_sf %>%
        filter(ortho_name==x) %>%
        st_set_crs(value = exts %>%
                       filter(image_name==x) %>%
                       pull(proj4) %>% unique)
    
    # Calculate 2 nearest neighbors that are less than 1 meters apart 
    # IDEA: could we use the 0.33 m cutoff here dirrectly saving computation time
    temp_nn<-temp_ortho %>% st_nn(y = temp_ortho,
                                  k = 2,
                                  maxdist = 1,
                                  returnDist = T,
                                  sparse = T)
    
    # reshape into a dataframe 
    temp_nn_df<-(as.data.frame(temp_nn$nn) %>%
                     t %>% 
                     as.data.frame) %>% 
        bind_cols(
            (as.data.frame(temp_nn$dist) %>%
                 t %>%
                 as.data.frame))
    
    names(temp_nn_df)<-c("idx1","idx2","dist1","dist")
    
    temp_nn_df<-temp_nn_df %>% select(idx1,idx2,dist)
    
    # plot distribution
    temp_nn_df$dist[temp_nn_df$dist>0.01] %>% hist(breaks=100)
    
    # Select the box with the lower box confidences to remove
    ids_to_drop<-temp_nn_df %>% 
        bind_cols(temp_ortho %>%
                      as.data.frame %>%
                      select(idx1_bc=box_confidence)) %>% 
        filter(dist<=0.33) %>% 
        left_join(temp_ortho %>%
                      as.data.frame() %>% 
                      select(idx2_bc=box_confidence) %>%
                      mutate(idx2=row_number())) %>% 
        mutate(idx=ifelse(idx1_bc<idx2_bc,idx1,idx2)) %>% 
        distinct(idx) %>%
        pull(idx)
    
    # drop the rows with the lower box confidence scores that are less then
    # 0.33m from the next penguin
    temp_ortho_nn<-temp_ortho[-ids_to_drop,] %>% 
        st_set_crs(NA)
    return(temp_ortho_nn)
})



```

### Create Shapefiles

We will now save a single shape file for each orthomosaic with all boxes from
all three models.

```{r}

exts2<-exts %>%
    mutate(ortho_name=gsub("_tiled-[0-9]{1,}-[0-9]{1,}","",image_name)) %>%
    distinct(ortho_name,proj4)

trash<-lapply(images,function(x){
    write_sf(top_per_ev_sf_nn %>%
                 filter(ortho_name==x) %>%
                 st_set_crs(value = exts2 %>%
                                filter(ortho_name==x) %>%
                                pull(proj4) %>% unique)
             ,paste0("R_Output/shps/",gsub(".JPG","_single_class_best.shp",x)))
})


```

```{r message=FALSE, warning=FALSE}
library(raster)
preds_sf<-read_sf(paste0("R_Output/shps/","croz_20201129_v1_crop_single_class_best.shp"))
st_crs(preds_sf)<-"+proj=longlat +ellps=WGS84 +no_defs"


ras<-raster::stack(exts$image_path[1])

detach("package:raster", unload = TRUE)

RStoolbox::ggRGB(ras)+
    geom_sf(data=preds_sf,aes(color=class),size=0.3)


```

### Make Summary Tables and Figures

```{r}

pen_count<-top_per_ev3 %>%
    group_by(ortho_name,class) %>%
    count %>% 
    mutate(colony=str_extract(ortho_name,"croz|royds"),
           area=str_extract(ortho_name,"east|west|WB_Area|royds"),
           date=str_extract(ortho_name,"[0-9]{6,}"),
           version=str_extract(ortho_name,"v[0-9]{1}"))%>% 
    mutate(date=ifelse(date=="2000112","20200112",date),
           date=lubridate::ymd(date),
           season=ifelse(date<"2020-06-01","2019-2020","2020-2021"),
           date_stan=if_else(season=="2020-2021",date-years(1),date)) %>%
    spread(class,n)

ggplot(pen_count %>% 
           mutate(area=ifelse(is.na(area),"west",area)) %>%
           pivot_longer(cols=starts_with("ADPE"),
                        names_to = "class",values_to="n")%>%
           ungroup %>%
           arrange(area,season,class,date_stan))+
    aes(x=class,y=n)+
    geom_col()+
    scale_color_viridis_d()+
    theme_bw()

ggsave("R_Output/penguin_count.jpg",width=8,height=6)

write.csv(pen_count,"R_Output/single_class_counts.csv",row.names = F)
```
